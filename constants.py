# -*- coding: utf-8 -*-

assignment1_instructions = """Paper 1
Intro Philosophy
3â€“5 pages (double-spaced, normal font and margins). First draft due 2/10, 10:20 am.
Please read all of these instructions carefully. This is a very focused assignment, based on one of the following selections from the textbook (choose one):
1. In Mackieâ€™s â€œEvil and Omnipotenceâ€, the paragraph on p. 124 that begins â€œFirst I should query...â€.
2. Kelly Clarkâ€™s article â€œWithout Evidence or Argumentâ€, two paragraphs on p. 166: the one beginning with â€œThe first problem...â€, and the next paragraph, which ends with â€œ...to see whyâ€.
Although your assignment will be based solely on the short selection, you should read the whole article (re-read, if you choose the Mackie selection) to be sure you understand the context in which the selection occurs. Your paper should begin with a short introductory paragraph that explains briefly to the reader the topic of your paper and what you plan to say. After that, your paper should extract, justify, and evaluate the argument in the selection. The articles contain much more than the short selections, but your paper should only extract, justify, and evaluate the argument in the selection. What do I mean by that? Extracting the argument means formulating an argument in numbered premise form, as Iâ€™ve done in class, that captures what the author (Mackie or Clark) is arguing. The argument you extract must be logically valid. This means that there can be no â€œsuppressed premisesâ€ omitted. For example, suppose you had to extract an argument from this paragraph:
People who cheat on their taxes should talk to my grandmother. My grandmother is a great woman. Sheâ€™s wise about all sorts of things, especially morality. Now, lots of people nowadays cheat on their taxes, but I know whose advice to take. Grandma was always clear: never cheat on your taxes!
A first attempt might be:
1. My grandmother is very wise
2. My grandmother says that cheating on your taxes is wrong
3. Therefore, cheating on your taxes is wrong
Notice that the conclusion of this argumentâ€”â€œcheating on your taxes is wrongâ€â€”is never explicitly stated in the paragraph. Thatâ€™s OK. The point of the paragraph is clearly to establish that cheating on your taxes is wrong. (Sometimes it can be hard to figure out exactly what the conclusion is supposed to be. The surrounding context of the selection can give you clues.)
The first attempt above at extracting the argument is unacceptable, since that argument is invalid. Premises 1 and 2 donâ€™t logically imply the conclusion, 3. In order to make the argument valid, a premise needs to be added:
1. My grandmother is very wise
2. My grandmother says that cheating on your taxes is wrong
3. If my grandmother says that cheating on your taxes is wrong, then cheating
on your taxes is wrong
4. Therefore, cheating on your taxes is wrong
(Alternatively, the new premise 3 might say that â€œeverything my grandmother says is trueâ€.) Premise 3 doesnâ€™t appear explicitly in the paragraph (itâ€™s a suppressed premise), but it is clearly being assumed, and it is needed to make the argument valid.
This second attempt is better, but still not perfect. Premise 1 isnâ€™t needed to make the argument valid, so it isnâ€™t serving any purpose in the argument. So it should be removed. Here is the final form of the argument:
1. My grandmother says that cheating on your taxes is wrong
2. If my grandmother says that cheating on your taxes is wrong, then cheating on your taxes is wrong
3. Therefore, cheating on your taxes is wrong
What happened to the claim that my grandmother is wise? Isnâ€™t that crucial? Well, that claim no longer appears in the argument, but it still plays a role. When you get to the justification stage (see below), youâ€™ll need to say why the defender of the argument thinks that premise 2 is true. And the answer is that the defender of the argument thinks that the grandmother is so wise that sheâ€™s bound to be right about the morality of cheating on taxes. One other thing about this. You may be tempted to incorporate grandmotherâ€™s being wise into the argument, by making premise 2 say:
2a. Since my grandmother is very wise, if she says that cheating on your taxes is wrong, then cheating on your taxes is wrong
or
2b. My grandmother is very wise, so [or: â€œthereforeâ€] if she says that cheating on your taxes is wrong, then cheating on your taxes is wrong Thatâ€™s not a good idea. For then, 2 would no longer be just a single premise.
Rather, it would be a little mini-argument all on its own. â€œMy grandmother is very wiseâ€ would be the premise of this mini-argument, and the conclusion would be â€œIf my grandmother says that cheating on your taxes is wrong, then cheating on your taxes is wrongâ€. (And notice that this mini-argument would have a suppressed premise!â€”namely, that â€œIf my grandmother is very wise, then if she says that cheating on your taxes is wrong, then cheating on your taxes is wrong.â€) So itâ€™s best to keep the argument the way it was above, i.e., not mentioning the grandmotherâ€™s wisdom at all. But if you really want to include a premise about the grandmotherâ€™s wisdom in the argument, you could do it this way: â€œ1. My grandmother says that cheating on your taxes is wrong. 2. My grandmother is very wise. 3. If my grandmother is very wise and says that cheating on your taxes is wrong, then cheating on your taxes is wrong. 4. Therefore, cheating on your taxes is wrong.â€
Justifying the argument requires going line by line through the argument youâ€™ve extracted, and saying why the person who offered the argument thinks the premises are true. In the example above, the person presumably thinks that premise 1 (â€œMy grandmother says that cheating on your taxes is wrongâ€) is true because they have heard their grandmother say this. And they think that
premise 2 (â€œIf my grandmother says that cheating on your taxes is wrong, then cheating on your taxes is wrongâ€) is true because the grandmother is very wise (perhaps especially about morality). Note: you give the line-by-line justifications for the premises only, not for the conclusion. The conclusion is supposed to be supported by the premises, so it requires no further support.
Evaluating the argument means assessing whether the argument is sound. Hopefully you will have extracted a valid argument; if so, soundness will then just amount to whether the argumentâ€™s premises are true.
In the case of your assignment, the authors (Mackie, Clark) of course think the premises are true; the question is whether they are right.
There are a number of different things you can do in the evaluation section. One thing you could do is make an objection to one of the premises. But what if you agree with the premises? In that case you could think of an objection that someone else might make to one of the premises, and discuss how the author (or you) might reply to that objection.
It is important that any objection to the argument must concern a specific premise. You might say, for example, â€œI think that premise 2 is false because...â€.
Finally, although the point of evaluation is to make your own contribution to the debate, you shouldnâ€™t merely express your feelings or opinions. You should give reasons for what you say."""

assignment1_instructions_compressed = u"""P1IP, 3-5pgs, 1st dft 2/10 10:20. Rd instrctns. Fcsd task, Mcki 'EO' p.124 strts 'F1st...' or K.Clark 'WEorA' two para p.166 strts '1st prob..' ends 'to see why'. Reread full txt, context comprehension. Intro para: paper topic, plan. Task: extract, justify, evaluate argument in selection, in numbered premise form, logically valid, no suppressed premises omitted. Ex: text -- > tax cheat wrong advice by grandma, extrctd argmnt unacceptbl; 1. GM wise 2. GM says tax cheat wrong 3. Hence, tax cheat wrong. Argmnt invalid, added premise: 1. GM wise, 2. GM says t. cht wrong, 3. If GM says t. cht right, then t. cht right, 4. T. cheat wrong (alt: GM truth). 'GM wise' removed from final argument: 1. GM says 't. cht wrong,' 2. If GM says 't. cht wrong,' t. cht wrong, 3. T. cht wrong. Wisdom of GM? Not explicit in argument but needed in justification stage explaining why premise 2 is true: GM wisdom --> right about tax morality. Justification: line by line reasons validity of premises. Evaluation: argument soundness. Aka authors think premises true but are they? Evaluation options: objection to a premise, addressing someone else's objection, or a response to that objection. Objection must target specific premise with reasons. No opinion - debate contribution needs premises."""

assignment1_option1_excerpt = """First I should query the assumption that second order evils are logically necessary accompaniments of freedom. I should ask this: if God has made men such that in their free choices they sometimes prefer what is good and sometimes what is evil, why could he not have made men such that they always freely choose the good? If there is no logical impossibility in a manâ€™s freely choosing the good on one, or on several occasions, there cannot be a logical impossibility in his freely choosing the good on every occasion. God was not, then, faced with a choice between making innocent automata and making beings who, in acting freely, would sometimes go wrong: there was open to him the obviously better possibility of making beings who would act freely but always go right. Clearly, his failure to avail himself of this possibility is inconsistent with his being both omnipotent and wholly good."""

assignment1_option1_excerpt_compressed = u"""1.qry2ndordrEvils&FrdmAssumptn. If ğŸš¹frChsGdEvil,whyNotğŸš¹alwaysFrChsGd?NoLogImposs. God'sChoice-NtOnlyInnocAutomata/ğŸš¹GoWrg, butAlsğŸš¹FrActAlwaysRight. NotUsingPosib=Incon2Omnipotent&whllyGd."""

assignment1_option2_excerpt = """The first problem with Cliffordâ€™s universal demand for evidence is that it cannot meet its own demand. Clifford offers two fetching examples (a shipowner who knowingly sends an unseaworthy ship to sea and, in the first example, it sinks and, in the second example, it makes the trip) in support of his claim. The examples powerfully demonstrate that in cases like the example, rational belief requires evidence. No one would disagree: some beliefs require evidence for their rational acceptability. But all beliefs in every circumstance? Thatâ€™s an exceedingly strong claim to make and, it turns out, one that cannot be based on evidence.
Consider what someone like Clifford might allow us to take for evidence: beliefs that we acquire through sensory experience and beliefs that are self-evident like logic and mathematics. Next rainy day, make a list of all of your experiential beliefs: The sky is blue, grass is green, most trees are taller than most grasshoppers, slugs leave a slimy trail.â€¦Now add to this list all of your logical and mathematical beliefs: 2 + 2 = 4, every proposition is either true or false, all of the even numbers that I know of are the sum of two prime numbers, in Euclidean geometry the interior angles of triangles equal 180. From these propositions, try to deduce the conclusion that it is wrong, always and everywhere, for anyone to believe anything on insufficient evidence. None of the propositions that are allowed as evidence have anything at all to do with the conclusion. So Cliffordâ€™s universal demand for evidence cannot satisfy its own standard! Therefore, by Cliffordâ€™s own criterion, it must be irrational. More likely, however, the demand is simply false and it is easy to see why."""

assignment1_option2_excerpt_compressed = u"""1st issue w/ Clifford's universal evidence demand: can't meet own demand. Gives 2 exmps (shipowner knowingly sends unfit ship, 1st sinks, 2nd completes trip) showcasing need for evidence in rational belief. Beliefs need evidence for rational acceptability âœ”ï¸, but all cases? Too strong a claim, can't be evidence-based. Clifford might allow sensory & self-evident (logic, math) beliefs as evidence. E.g. sky blue, grass green, 2+2=4, prop true/false, even num = 2 primes sum, etc. Try deduce: wrong to believe w/o enough evidence always. Used evidence not relatable to conclusion. Thus, Clifford's demand fails at own standard, is irrational. Likely, demand just false."""

assignment2_instructions = """Paper 2
Intro Philosophy
5â€“7 pages (double-spaced, normal font and margins).
This is another focused assignment, though broader in scope, in which you must extract, justify, and evaluate an argument from a text. You should re-read the instructions from paper 1 on extraction, justification, and evaluation. Be especially careful in the evaluation phase to stay focused. Your evaluation should not consist of general musings on the subject matter. Rather, you should, e.g., consider a specific objection to the argument you have presented, and you should back up what you say with clear reasons. Although your assignment will be based on a relatively short selection, you ought to read the whole article containing the selection to be sure you understand the context in which the selection occurs. Choose one of the following assignments:
1. In section 2 of his article, â€œWhyWe Have No FreeWill and Can Live Without Itâ€ (which is in the textbook), Derk Pereboom gives an extended argument against compatibilism. Begin your paper by introducing the issuesâ€”quickly explain the problem of freedom and determinism and say what compatibilism is. Then, extract, justify, and evaluate Pereboomâ€™s argument against compatibilism. The argument you extract should capture the entirety of the main line of thought Pereboom offers in the section. In the justification phase (when you go through the argument you have extracted and say, line-by-line, why Pereboom thinks the premises are true) you will need toâ€”brieflyâ€”describe each of Pereboomâ€™s four cases.
2. Doctors are never allowed to kill patients, not even when the patient is suffering from a painful incurable disease. But in certain cases, doctors are allowed to let patients die (by withholding care that would otherwise prolong their lives). In his article â€œActive and Passive Euthenasiaâ€ (which is in the textbook), James Rachels argues that this is unreasonable. He argues that either doctors should never be allowed to cause patients to die (whether by killing them or letting them die), or they should be allowed both to kill patients (only in certain highly regulated situations of course) as well as let them die. You should read the whole article, but your paper should focus on the four paragraphs on p. 685 in which Rachels argues that the distinction between killing and letting die is not a morally important one. The first of these paragraphs starts with â€œOne reason why. . . â€; the fourth paragraph ends with the sentence â€œMorally speaking, it is no defense at all.â€
After an introductory paragraph or two in which you introduce the main issues, your assignment is to extract, justify, and evaluate the argument in these four paragraphs."""

assignment2_instructions_compressed = u"""P2 Intro Philo 5-7pg dbl-sp, focsd asgmnt, mst xtrct, jstfy, eval agmnt frm txt. Re-rd inst frm p1, stay focsd ein eval. Not gen musings, csdr objctn, bkp wd clr rsns. Rd ful txt fr cntxt. Asgmnts:
1. S2 Derk Pereboom's No FreeWill textb. Intro issu, qkly expln freed and determ, compat. Xtrct,jstfy,eval agnst compat agmnt. Capt main thoughtln, jstfy phse: brfly dscrib 4cases.
2. Docs nvr kill ptnts, ptntltn 2 let ptnt die alwd (ws trade-off care). J. Rachels' Active&Passive Euth textb, acpts unrsnble. Argu either nvr cuz ptnt 2die or alwd in reglted situations. R&W whole, fcus four para. p. 685, argue killing&letting die nt morally imp. Start paran "One reason why", ends â€œit is no defense at allâ€.
Intro main issues, xtrct, jstfy, eval these 4para's agmnt."""

assignment2_option1_excerpt = """The case for hard incompatibilism involves arguing against two competing positions. The first of these is compatibilism, which claims that free will of the type required for moral responsibility is compatible with determinism. Compatibilists typically maintain, in addition, that we do in fact have this sort of free will. The second is libertarianism, which contends that although the sort of free will required for moral responsibility is not compatible with determinism, it turns out that determinism is false, and we do have this kind of free will. 
Compatibilists typically attempt to formulate conditions on agency intended to provide an account of what it is to be morally responsible for an action. These conditions are compatibilist in that they allow for an agent to be morally responsible for an action even when she is causally determined to act as she does. For instance, David Hume and his followers specify that morally responsible action be caused by desires that flow from the agentâ€™s â€œdurable and constantâ€ character, and that the agent not be constrained to act, at least in the sense that the action not result from an irresistible desire (Hume 1739/1978: 319â€“412). Harry Frankfurt proposes that moral responsibility requires that the agent have endorsed and produced her will to perform the action in the right way. More specifically, she must have a second-order desireâ€”that is, a desire to have a particular desireâ€”to will to perform it, and her will must be her will because she has this second-order desire (Frankfurt 1971). John Fischer argues that morally responsible action must result from a rational consideration of the reasons at issue; among other things, the agent must be receptive to the reasons present in a situation, and she must be responsive to them to the degree that in at least some situations in which the reasons are different, she would have done otherwise (Fischer 1994). Finally, Jay Wallace proposes that moral responsibility requires that the agent have the general ability to grasp, apply, and regulate her behavior by moral reasons (Wallace 1994). Each of these compatibilists intends for his conditions to be sufficient for an agentâ€™s moral responsibility when they are supplemented by some fairly uncontroversial additional necessary conditions, such as the provision that the agent understands that killing is morally wrong. 
In my view, the best type of challenge to the compatibilist begins with the intuition that if someone is causally determined to act by other agents, for example, by scientists who manipulate her brain, then she is not morally responsible for that action. This intuition remains strong even if she meets the compatibilist conditions on moral responsibility just canvassed. The following â€œfour-case argumentâ€ first of all develops examples of actions that involve such manipulation, in which these compatibilist conditions on moral responsibility are satisfied. Manipulation cases, taken individually, indicate that it is possible for an agent not to be morally responsible even if the compatibilist conditions are satisfied, and that as a result these conditions are inadequate. But the argument has additional force, by way of setting out three such cases, each progressively more like a fourth scenario which the compatibilist would regard as realistic, in which the action is causally determined in a natural way. An additional challenge for the compatibilist is to point out a relevant difference between any two adjacent cases that would show why the agent might be morally responsible in the later example but not in the earlier one. I argue that this canâ€™t be done. So I contend that the agentâ€™s non-responsibility generalizes from the first two manipulation examples to the ordinary case. 
In each of the four cases, Professor Plum decides to kill Ms. White for the sake of some personal advantage, and succeeds in doing so. We design the cases so that his act of murder conforms to the prominent compatibilist conditions. Plumâ€™s action meets the Humean conditions, since for him purely selfish reasons typically weigh heavilyâ€”much too heavily as judged from the moral point of view, while in addition the desire that motivates him to act is nevertheless not irresistible for him, and in this sense he is not constrained to act. It fits the condition proposed by Frankfurt: Plumâ€™s effective desire (i.e., his will) to murder White conforms appropriately to his second-order desires for which effective desires he will have. That is, he not only wills to murder her, but he also wants to will to do so. The action also satisfies the reasons-responsiveness condition advocated by Fischer: Plumâ€™s desires are modified by, and some of them arise from, his rational consideration of the reasons he has, and if he knew that the bad consequences for himself that would result from killing White would be much more severe than they are actually likely to be, he would have refrained from killing her for this reason. In addition, this action meets the condition advanced by Wallace: Plum retains general ability to grasp, apply, and regulate his behavior by moral reasons. When egoistic reasons that count against acting morally are relatively weak, he will usually regulate his behavior by moral reasons instead. This ability provides him with the capacity to revise and develop his moral character over time, as Alfred Mele (1995, 2006)
requires. Now, supposing that causal determinism is true, is it plausible that Plum is morally responsible for his action? 
Each of the four cases features different ways in which Plumâ€™s murder of White might be causally determined by factors beyond his control. 
Case 1: A team of neuroscientists is able to manipulate Professor Plumâ€™s mental state at any moment through the use of radio-like technology. In this case, they do so by pressing a button just before he begins to reason about his situation. This causes Plumâ€™s reasoning process to be egoistic, which the neuroscientists know will deterministically result in his decision to kill White. Plum does not think and act contrary to character since his reasoning processes are not infrequently egoistic. His effective first-order desire to kill White conforms to his second-order desires. The process of deliberation from which his action results is reasons-responsive; in particular, this type of process would have resulted in his refraining from killing White in some situations in which the reasons were different. Still, his reasoning is not in general exclusively egoistic, since he often regulates his behavior by moral reasons, especially when the egoistic reasons are relatively weak. He is also not constrained, in the sense that he does not act because of an irresistible desireâ€”the neuroscientists do not induce a desire of this kind. 
In Case 1, Plumâ€™s action satisfies all the compatibilist conditions we just examined. But intuitively, he is not morally responsible for the murder, because his action is causally determined by what the neuroscientists do, which is beyond his control. Consequently, it would seem that these compatibilist conditions are not sufficient for moral responsibilityâ€”even if all taken together. A compatibilist might resist this conclusion by arguing that although in Case 1 the process resulting in the action satisfies all of the prominent compatibilist conditions, yet Plumâ€™s relevant states are directly produced by the manipulators at the time of the actionâ€”he is locally manipulatedâ€”
and this is the aspect of the story that undermines his moral responsibility. In reply, if the neuroscientists did all of their manipulating during one time interval and, after some length of time, the relevant states were produced in him, would he only then be morally responsible? It is my sense that such a time lag, all by itself, would make no difference to whether an agent is responsible. So let us now consider a scenario in which the manipulation takes place long before the action: 
Case 2: Plum is like an ordinary human being, except that neuroscientists have programmed him at the beginning of his life so that his reasoning is frequently but not always egoistic (as in Case 1), with the consequence that in the particular circumstances in which he now finds himself, he is causally determined to engage in the egoistic reasons-responsive process of deliberation and to have the set of first- and secondorder desires that result in his decision to kill White. Plum has the general ability to regulate his behavior by moral reasons, but in his circumstances, due to the egoistic character of his reasoning, he is causally determined to make his decision. At the same time, he does not act because of an irresistible desire. 
Here again, although Plum meets each of the compatibilist conditions, it is intuitive that he is not morally responsible. Thus Case 2 also shows that the prominent compatibilist conditions, either separately or in conjunction, are not sufficient for moral responsibility. Again, it would seem unprincipled to claim that here, by contrast with Case 1, Plum is morally responsible because the length of time between the programming and the action is now great enough. Here also it would seem that he is not morally responsible because he is causally determined to decide and act by forces beyond his control. 
Imagine next a scenario more similar yet to an ordinary situation: 
Case 3: Plum is an ordinary human being, except that he was causally determined by the rigorous training practices of his household and community in such a way that his reasoning processes are often but not exclusively rationally egoistic (as in Cases 1 and 2). This training took place when he was too young to have the ability to prevent or alter the practices that determined this aspect of his character. This training, together with his particular current circumstances, causally determines him to engage in the egoistic reasonsresponsive process of deliberation and to have the first- and second-order desires that result in his decision to kill White. Plum has the general ability to regulate his behavior by moral reasons, but in his circumstances, due to the egoistic nature of his reasoning processing, he is causally determined make his decision. Here again his action is not due to an irresistible desire. 
If a compatibilist wishes to contend that Plum is morally responsible in Case 3, he needs to point to a feature of these circumstances that would explain why he is morally responsible here but not in Case 2. But it seems that there is no such feature. In each of these examples, Plum meets all the prominent compatibilist conditions for morally responsible action, so a divergence in judgment about moral responsibility between these examples will not be supported by a difference in whether these conditions are satisfied. Causal determination by factors beyond his control most plausibly explains the absence of moral responsibility in Case 2, and we are constrained to conclude that Plum is not morally responsible in Case 3 for the same reason. 
Thus it appears that Plumâ€™s exemption from responsibility in Cases 1 and 2 generalizes to the nearer-to-normal Case 3. Does it generalize all the way to the ordinary case? 
Case 4: Physicalist determinism is trueâ€”everything in the universe is physical, and everything that happens is causally determined by virtue of the past states of the universe in conjunction with the laws of nature. Plum is an ordinary human being, raised in normal circumstances, and again his reasoning processes are frequently but not exclusively egoistic (as in Cases 1â€“3). His decision to kill White results from his reasonsresponsive process of deliberation, and he has the specified first- and second-order desires. Again, he has the general ability to grasp, apply, and regulate his behavior by moral reasons, and his action is not due to an irresistible desire. 
Given that we need to deny moral responsibility in Case 3, could Plum be responsible in this more ordinary case? There would seem to be no differences between Case 3 and Case 4 that could serve to justify the claim that Plum is not responsible in Case 3 but is in Case 4. One distinguishing feature of Case 4 is that the causal determination of Plumâ€™s crime is not brought about by other agents (Lycan 1997: 117â€“8). However, the claim that this is a relevant difference is implausible. Imagine a further example that is exactly the same as, say, Case 1 or Case 2, except that Plumâ€™s states are induced by a spontaneously generated machineâ€”a machine that has no intelligent designer. Here also Plum would not be morally responsible. 
The best explanation for the intuition that Plum is not morally responsible in the first three cases is that his action is produced by a deterministic causal process that traces back to factors beyond his control. Because his action is also causally determined in this way in Case 4, we should conclude that here again he is not morally responsible. So by this argument, Plumâ€™s non-responsibility in Case 1 generalizes to nonresponsibility in Case 4. We should conclude that if an action results from any deterministic causal process that traces back to factors beyond the agentâ€™s control, then she will lack the control required to be morally responsible for it.
"""

assignment2_option1_excerpt_compressed = u"""Incomp argmt hinges on compatib&libertar. Compatib:say free will req for moral resp is compat w/ determinism&we do hv this free will. Libertar: even if this free will is not compat w/ determinism,determinism is false&we hv this kind of free will.

Compatibilist condition by: Hume-desire from agentâ€™s constant char. agent not constrained; Frankfurt-moral resp req agent's endorsement&will production in right way,2nd-order desire needed; Fischer-action frm rational consideration of reasons, agent receptive&responsive; Wallace-needs ability to grasp, apply,regulate behavior by moral reasons.

Challenge: If causally determin by others, agent is not responsible even if meets compatib conditions. 4-case argmt develop actions invlv manipulation that met compatib conditions but argues conditions could be inadequate as each cases move closer to realism. Shftg moral resp frm manipulation cases to ordinry cases.

Prof. Plum's 4 cases fit compatibilist conditions: his murder derived from selfish reasons, not constrained, fits Frankfurt's condition&Fischer's condition&Wallace's condition. Is Plum resp in causal determinism?

4 variants of Plumâ€™s murder determin by uncontrol factors. Case 1: neurosci manipulate Plum's mental state&reasoning through radio-like tech causing egoistic reasoning leading to murder. Case 2: Plum is normal human except neurosci programmed him since birth to possess frequently egoistic reasoning leading to murder. Case 3: Plum's reasoning was shaped through his upbringing leading to egoistic murdering. Case 4: Physicalist determinism applies, everything in the universe is causal determin, Plumb is stereotypical human raised in normal environments, his murder due to his egoistic reasoning.

Plum satisfies all compatib condition in all cases but seems not responsible because action is determined by external factors. Concluding from Case 3 to 4, changes in Plum's circumstances don't seem to justify responsibility in the latter case. Thus non-responsibility in Case 1 is generalized until Case 4 concluding that any action from determin causal process causes lack of control required for moral responsibility."""

assignment2_option2_excerpt = """One reason why so many people think that there is an important moral difference between active and passive euthanasia is that they think killing someone is morally worse than letting someone die. But is it? Is killing, in itself, worse than letting die? To investigate this issue, two cases may be considered that are exactly alike except that one involves killing whereas the other involves letting someone die. Then, it can be asked whether this difference makes any difference to the moral assessments. It is important that the cases be exactly alike, except for this one difference, since otherwise one cannot be confident that it is this difference and not some other that accounts for any variation in the assessments of the two cases. So, let us consider this pair of cases: In the first, Smith stands to gain a large inheritance if anything should happen to his sixyear-
old cousin. One evening while the child is taking his bath, Smith sneaks into the bathroom and drowns the child, and then arranges things so that it will look like an accident. In the second, Jones also stands to gain if anything should happen to his six-year-old cousin. Like Smith, Jones sneaks in planning to drown the child in his bath. However, just as he enters the bathroom Jones sees the child slip and hit his head, and fall face down in the water. Jones is delighted; he stands by, ready to push the childâ€™s head back under if it is necessary, but it is not necessary. With only a little thrashing about the child drowns all by himself, â€œaccidentally,â€
as Jones watches and does nothing. Now Smith killed the child, whereas Jones â€œmerelyâ€ let the child die. That is the only difference between them. Did either man behave better, from a moral point of view? If the difference between killing and letting die were in itself a morally important matter, one should say that Jonesâ€™s behavior was less reprehensible than Smithâ€™s. But does one really want to say that? I think not. In the first place, both men acted from the same motive, personal gain, and both had exactly the same end in view when they acted. It may be inferred from Smithâ€™s conduct that he is a bad man, although that judgment may be withdrawn or modified if certain further facts are learned about himâ€”for example, that he is mentally deranged. But would not the very same thing be inferred about Jones from his conduct? And would not the same further considerations also be relevant to any modification of this judgment? Moreover, suppose Jones pleaded, in his own defense, â€œAfter all, I didnâ€™t do anything except just stand there and watch the child drown. I didnâ€™t kill him; I only let him die.â€ Again, if letting die were in itself less bad than killing, this defense should have at least some weight. But it does not. Such a â€œdefenseâ€
can only be regarded as a grotesque perversion of moral reasoning. Morally speaking, it is no defense at all."""

assignment2_option2_excerpt_compressed = u"""1R why mnyğŸ¤” imp. moral diff. btween act&pas euthan' is thinkg ğŸ”ª>LD. ğŸ”2 cases 1 w/ ğŸ”ª, other LD, â“their moral assess. Identical case except 1 diff ğŸ”ªvsLD. c1: Smith kills 6yo cousin 4 ğŸ’µ, make accdnt. c2: Jones plns ğŸ”ª6yo cousins 4ğŸ’µ. c2F, kid slip hitsğŸ¤•, ğŸ’§alone, JonesğŸ‘€&do nthg, "accidentally" ğŸ‘¦ğŸ’€. SğŸ”ª, J "let"ğŸ‘¦ğŸ’€. 1 diff. Fairer, morally? ğŸ”ª or LD itself moral matter, J<S. Really? Both bad motive, same ğŸ”š. S=badman, may change if he's crazy. J=badman too. No change in judging. J: â€œI jst ğŸ‘€himğŸ’€. I dn'tğŸ”ª, I jst let." If LD<ğŸ”ª, should weight this def. But no. Such "def"ğŸš«moral reason. No def at all, moral wise."""

assignment1_prompt_blueprint = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and then give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing 
Clarity:  /10
Grammar:  /5

Overall: /100 (Sum of all the above)

[Short comment on ways to improve the paper]
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you a part of the paper (Extraction/Justifications/Evaluation) that you will have to grade.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper. Here is the student paper:

***
{student_paper}
***

Ok. Now grade the paper."""

assignment1_prompt_blueprint_compressed = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and then give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing 
Clarity:  /10
Grammar:  /5

Overall: /100 (Sum of all the above)

[Short comment on ways to improve the paper]
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you a part of the paper (Extraction/Justifications/Evaluation) that you will have to grade.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper. Here is the student paper, compressed by another instance of GPT-4:

***
{student_paper}
***

Ok. Now grade the paper."""

assignment1_prompt_blueprint_pdf = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and then give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing 
Clarity:  /10
Grammar:  /5

Overall: /100 (Sum of all the above)

[Short comment on ways to improve the paper]
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you a part of the paper (Extraction/Justifications/Evaluation) that you will have to grade.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper as a PDF."""

assignment2_prompt_blueprint = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing
Clarity:  /10
Grammar:  /5

Overall: /100 (Sum of all the above)

[Short comment on ways to improve the paper]
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you a part of the paper (Extraction/Justifications/Evaluation) that you will have to grade.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

There instructions make reference to 'paper 1 instructions'. Here are the paper 1 instructions, compressed by another instance of GPT-4:

***
{extra}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper. Here is the student paper:

***
{student_paper}
***

Ok. Now grade the paper."""

assignment2_prompt_blueprint_compressed = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing
Clarity:  /10
Grammar:  /5

Overall: /100 (Sum of all the above)

[Short comment on ways to improve the paper]
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you a part of the paper (Extraction/Justifications/Evaluation) that you will have to grade.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

There instructions make reference to 'paper 1 instructions'. Here are the paper 1 instructions, compressed by another instance of GPT-4:

***
{extra}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper. Here is the student paper, compressed by another instance of GPT-4:

***
{student_paper}
***

Ok. Now grade the paper."""

assignment2_prompt_blueprint_pdf = """Hi, I need you to grade a philosophy assignment. Make your best effort to grade it according to the rubric and give a short comment on the ways it can be improved. Here is a rubric for grading:

***
Introductory paragraph:  /10

Extraction
Validity:  /10 (i.e., do the premises of the argument logically entail the conclusion?)
Accuracy: /15   (i.e., does their argument accurately reflect the paragraph(s)?)

Justifications
Structure: /10  (each premise individually justified?  justified just the premises (not the conclusion)?)
Content:  /15  (are the justifications on-track?)

Evaluation
Structure:  /10  (does what they say have the right form of an argument evaluation--i.e. evaluating whether the premises are true?)
Content:  /15  (is the evaluation accurate?)

Quality of Writing
Clarity:  /10
Grammar:  /5
***

I am then going to send you the assignment instructions and whatever information is required to understand the instructions and do the assignment. Finally, I am going to send you the paper that you have to grade as a PDF.

Here are the assignment instructions, compressed by another instance of GPT-4:

***
{assignment}
***

There instructions make reference to 'paper 1 instructions'. Here are the paper 1 instructions, compressed by another instance of GPT-4:

***
{extra}
***

Here are the relevant pieces of text for assignment, compressed by another instance of GPT-4:

[Option 1]
***
{assignment_option_1}
***

[Option 2]
***
{assignment_option_2}
***

Now I am going to send you the student paper as a PDF."""